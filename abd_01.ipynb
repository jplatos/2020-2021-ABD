{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bit338dbb2cb8464efc9e0e4a7e5c7dfcb4",
   "display_name": "Python 3.7.3 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms for Big Data - Exercise 1\n",
    "This lecture is about basics of the tensorflow, we will discuss the minimal example on the MNIST dataset.\n",
    "\n",
    "We also investigate a meaning of the validation sets and different complexity of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Open in Google colab](https://colab.research.google.com/github/jplatos/2020-2021-ABD/blob/master/abd_01.ipynb)\n",
    "[Download from Github](https://raw.githubusercontent.com/jplatos/2020-2021-ABD/master/abd_01.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import of the TensorFlow\n",
    "The main version of the TensorFlow (TF) is a in the Version package in the field VERSION\n",
    "Since the TensformFlow 2.0 everything was encapsulaed under the KERAS api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import a dataset\n",
    "Datasets are stored in the keras.datasets submodule. Few testing datasets are stored here and installed together with the TF package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mnist is the basic dataset with handwritten digits\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# data from any dataset are loaded using the load_Data function\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# the data are in the form of 28x28 pixes with values 0-255.\n",
    "print('Train data shape: ', x_train.shape)\n",
    "print('Test data shape:  ', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets look on the data how do they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(x_train[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The conversion into range 0-1 is done by the division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The conversion into range 0-1 is done by the division\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make better visualization of the data to better understand how complex they are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [str(x) for x in range(10)]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_train[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[y_train[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic model - a NN with very simple hierarchy\n",
    "Model is created using layers, many layers exists in the [layer submodule](https://www.tensorflow.org/api_docs/python/tf/keras/layers)\n",
    "\n",
    "Each layer uses a activation functions collected in the [module nn](https://www.tensorflow.org/api_docs/python/tf/nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),# Flatten module flatten the multidimension input into single vector 28x28 = 784 float numbers\n",
    "    keras.layers.Dense(32, activation=tf.nn.relu), # standard dense-fully connected layer with the rectified lineaar function as an activation\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax), # another fully-connected layer with softmax activation function\n",
    "])\n",
    "\n",
    "model.summary() # prints the summary of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilation of the model\n",
    "Each model need to be compiled to be able to fit to the data and predict the labels\n",
    "\n",
    "#### Optimizers\n",
    "* Gradient descent\n",
    "   * Works for the whole dataset and it is not suitable for large data\n",
    "* Stochastic Gradiend Descent (SGD)\n",
    "   * Approximate the real gradiend from selested subset of data (Stochasticity)\n",
    "* Root Mean Square Propagation (RMSPRop)\n",
    "   * Adapts the learnign rate with the running average of the recent gradients.\n",
    "* Adamptive Moment Estimation (ADAM)\n",
    "   * Averages gradients and secodn moment of the gradient and adapts the learning rate.\n",
    "\n",
    "#### Loss functions\n",
    "* Mean Squared Error\n",
    "   * a classical measure to be used in regression\n",
    "   * a logarithmic version exists\n",
    "* Mean Absolute Error (MAE)\n",
    "   * take an absolute values instead of their squared version\n",
    "* Binary classification Loss\n",
    "   * a loss for binary problems only\n",
    "   * predicts the probability of the class 1\n",
    "* Binary Cross-Entropy\n",
    "   * predict the class from the set {0,1}\n",
    "   * requires a sigmoind activation function\n",
    "* Categorical Cross-Entropy\n",
    "   * default for mutli-class classification problems\n",
    "   * requires the softmax function on output layer to compute probability of each layer\n",
    "   * train labels have to be one-hot-encoded\n",
    "* Sparse Categorical Cross-Entropy\n",
    "   * the same as above but the tran lables are just labels not encoded.\n",
    "\n",
    "#### Metrics\n",
    "* Regression metrics\n",
    "   * Mean Squared Error (MSE)\n",
    "   * Mean Absolute Error (MAE)\n",
    "   * Mean Absolute Percentage Error (MAPE)\n",
    "* Classification metrics\n",
    "   * Binary Accuracy\n",
    "   * Categorical Accuracy\n",
    "   * Sparse Categorical Accuracy\n",
    "   * Top k Categorical Accuracy\n",
    "   * Sparse Top k Categorical Accuracy\n",
    "   * Accuracy - a general version that is modified based on the data analyzed autmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics = ['accuracy']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model visualization\n",
    "Model then compiles and it is ready for fitting to the data. \n",
    "The model may be printed into image like the following image of our model:\n",
    "\n",
    "![model](https://github.com/jplatos/2019-2020-DA4/raw/master/images/da4_01_base.png \"Base model of the neural network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model to the input data\n",
    "The *fit()* method fit the model to the data, the parameters are *data* and *labels* from the train set and number of *epoch* to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for key in history.history.keys():\n",
    "    plt.plot(history.epoch, history.history[key], label=key)\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks for the rest of the exercise\n",
    "\n",
    "1. Investigate the quality of the model using validation sets.\n",
    "2. Limit the overfitting using the regularization and dropout.\n",
    "   1. L1 regularization (Lasso regression) - $\\lambda_1 \\sum_{i=0}^n \\left\\lvert w_i\\right\\rvert$\n",
    "   2. L2 regularization (Ridge regression) - $\\lambda_2 \\sum_{i=0}^n \\left\\lVert w_i\\right\\rVert$\n",
    "   3. Dropout\n",
    "3. Prepare its own model that will classifiy the test data with precision more than 98 percent."
   ]
  }
 ]
}